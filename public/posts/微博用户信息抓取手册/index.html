<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>微博用户信息抓取手册 | JW&#39;s Log</title>
<meta name="keywords" content="爬虫, 微博">
<meta name="description" content="主要介绍如何通过TODESK和SSH连接实验室的服务器系统">
<meta name="author" content="">
<link rel="canonical" href="https://AquariniqueMu.github.io/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.57dd179325b0530c05a692ea0b60b140fcab2263bdab46b6c468212398dc878a.css" integrity="sha256-V90XkyWwUwwFppLqC2CxQPyrImO9q0a2xGghI5jch4o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://AquariniqueMu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://AquariniqueMu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://AquariniqueMu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://AquariniqueMu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://AquariniqueMu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://AquariniqueMu.github.io/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style><meta property="og:title" content="微博用户信息抓取手册" />
<meta property="og:description" content="主要介绍如何通过TODESK和SSH连接实验室的服务器系统" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://AquariniqueMu.github.io/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-13T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="微博用户信息抓取手册"/>
<meta name="twitter:description" content="主要介绍如何通过TODESK和SSH连接实验室的服务器系统"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://AquariniqueMu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "微博用户信息抓取手册",
      "item": "https://AquariniqueMu.github.io/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "微博用户信息抓取手册",
  "name": "微博用户信息抓取手册",
  "description": "主要介绍如何通过TODESK和SSH连接实验室的服务器系统",
  "keywords": [
    "爬虫", "微博"
  ],
  "articleBody": "微博用户爬虫项目 本项目是一个用于批量抓取微博用户微博数据的爬虫工具，支持从Excel文件中读取用户ID，抓取指定时间范围内的微博内容、图片等信息，并将结果保存到本地。现在，我们已经将项目封装为Docker容器，并开放了一个API接口，方便他人部署和使用。\n地址：https://github.com/AquariniqueMu/WeiboUserCrawler\n目录 项目结构 环境依赖 配置文件 config.py 运行爬虫 1. 使用Docker部署和运行 1.1 构建Docker镜像 1.2 运行Docker容器 1.3 调用API接口 1.4 查看结果 2. 直接运行Python脚本 2.1 抓取单个用户微博数据 2.2 批量抓取多个用户微博数据 数据存储 注意事项 项目文件详解 常见问题 联系方式 免责声明 requirements.txt 项目结构 WeiboUserCrawler/ ├── app.py ├── config.py ├── Dockerfile ├── main_from_xlsx.py ├── main_single_user.py ├── requirements.txt ├── utils.py ├── weibo.py └── weibo_results/ (empty, will be populated during runtime) app.py：Flask应用入口，提供API接口。 config.py：配置文件，包含默认参数。 Dockerfile：Docker镜像构建文件。 main_from_xlsx.py：从Excel文件中读取用户ID，批量抓取微博数据的脚本。 main_single_user.py：抓取单个用户微博数据的脚本。 requirements.txt：Python依赖库列表。 utils.py：工具函数，包含数据类型处理和日期转换等功能。 weibo.py：微博爬虫核心代码，定义了WeiboCrawler类。 weibo_results/：默认的数据保存目录，抓取的结果会存储在此文件夹中。 项目下载 git clone https://github.com/AquariniqueMu/WeiboUserCrawler.git cd WeiboUserCrawler 环境依赖 Python 版本：3.12.5\n依赖库：\nrequests\u003e=2.31.0 pandas\u003e=2.0.3 tqdm\u003e=4.65.0 openpyxl\u003e=3.1.2 Flask\u003e=2.0.3 可使用以下命令安装依赖：\npip install -r requirements.txt 配置文件 config.py 在使用爬虫前，需要配置config.py文件，包括Cookie、抓取时间范围等。\nfrom utils import convert_date_to_timestamp # 默认的Cookie，可在运行时通过API传入 cookie = \"\" result_dir = './weibo_results' start_date = convert_date_to_timestamp('2023-01-01') # 开始时间，格式：YYYY-MM-DD end_date = convert_date_to_timestamp('2023-12-31') # 结束时间，格式：YYYY-MM-DD download_images = True # 是否下载图片 fetch_reposts = True # 是否抓取转发微博 运行爬虫 1. 使用Docker部署和运行 通过Docker容器化，您可以轻松地在任何支持Docker的环境中部署和运行本项目。\n1.1 构建Docker镜像 在项目根目录下，执行以下命令构建Docker镜像：\ndocker build -t weibo-crawler . 1.2 运行Docker容器 使用以下命令运行Docker容器，并将结果目录映射到宿主机：\ndocker run -d -p 5000:5000 -v /your/host/path/weibo_results:/app/weibo_results --name weibo-crawler weibo-crawler -d：以守护进程方式运行容器。 -p 5000:5000：将容器内的5000端口映射到宿主机的5000端口。 -v /your/host/path/weibo_results:/app/weibo_results：将宿主机的目录映射到容器内的/app/weibo_results，用于数据持久化。 --name weibo-crawler：为容器指定一个名称，方便管理。 **注意：**请将/your/host/path/weibo_results替换为您宿主机上用于存放结果的实际路径。\n1.3 调用API接口 使用curl命令或其他HTTP客户端（如Postman）发送POST请求：\ncurl -X POST http://localhost:5000/crawl \\ -H \"Content-Type: application/json\" \\ -d '{ \"user_id\": \"1234567890\", \"cookie\": \"YOUR_COOKIE_HERE\", \"start_date\": \"2023-01-01\", \"end_date\": \"2023-12-31\", \"download_images\": true, \"fetch_reposts\": true }' 参数说明：\nuser_id：必填，微博用户ID。 cookie：必填，您的微博Cookie。 start_date：选填，开始日期，格式YYYY-MM-DD。 end_date：选填，结束日期，格式YYYY-MM-DD。 download_images：选填，是否下载图片，默认为false。 fetch_reposts：选填，是否抓取转发微博，默认为false。 示例响应：\n{ \"message\": \"Started crawling for user 1234567890\" } 1.4 查看结果 抓取的结果会保存在宿主机的/your/host/path/weibo_results目录下，以用户ID命名的子目录中。\n查看容器日志：\ndocker logs -f weibo-crawler 停止容器：\ndocker stop weibo-crawler 启动容器：\ndocker start weibo-crawler 删除容器：\ndocker rm weibo-crawler 2. 直接运行Python脚本 如果您希望在本地直接运行Python脚本，可以按照以下步骤操作。\n2.1 抓取单个用户微博数据 修改main_single_user.py中的user_id为目标用户的ID，然后运行该脚本。\nfrom weibo import WeiboCrawler from config import cookie, result_dir, start_date, end_date, download_images, fetch_reposts def run_weibo_crawler(): user_id = '1234567890' # 替换为目标用户的ID cookie = 'YOUR_COOKIE_HERE' # 替换为你的Cookie config = { 'user_id': user_id, 'cookie': cookie, 'result_dir': result_dir, 'start_date': start_date, 'end_date': end_date, 'download_images': download_images, 'fetch_reposts': fetch_reposts, } print(\"=\"*50 + \" Start Weibo Crawler \" + \"=\"*50 + \"\\n\") weibo_crawler = WeiboCrawler(config) weibo_crawler.start() print(\"\\n\" + \"=\"*50 + \" End Weibo Crawler \" + \"=\"*50) if __name__ == \"__main__\": run_weibo_crawler() 运行命令：\npython main_single_user.py 2.2 批量抓取多个用户微博数据 将用户ID列表保存在Excel文件中，然后运行main_from_xlsx.py。\n准备Excel文件\n创建一个Excel文件（默认文件名：user_ids.xlsx）。 在第一列（默认列名：用户ID）中填写用户ID列表。 运行脚本\npython main_from_xlsx.py --excel_file your_excel_file.xlsx --id_column 用户ID --excel_file：Excel文件路径，默认值为user_ids.xlsx。 --id_column：用户ID列名，默认值为用户ID。 示例：\npython main_from_xlsx.py 如果使用默认的文件名和列名，可以直接运行上述命令。\n数据存储 所有抓取的结果会存储在weibo_results/目录下。 每个用户的数据会存放在以用户ID命名的子目录中。 数据包括： 微博内容CSV文件：weibo_用户ID.csv 图片文件：存储在对应微博ID的images/目录下。 注意事项 合法性与合规性 请遵守微博的服务条款和相关法律法规。 本工具仅供学习和研究使用，禁止用于任何商业或非法用途。 请勿过度抓取，以免给微博服务器带来压力。 关于Cookie 安全性：Cookie包含敏感信息，请妥善保管，避免泄露。 时效性：Cookie可能会过期，若爬虫无法正常工作，请尝试更新Cookie。 请求频率 为避免触发微博的反爬机制，程序中已设置了随机等待时间。 如需调整，请在weibo.py的start方法中修改time.sleep的参数。 多线程与并发 Docker部署的API接口中，每个爬虫任务在一个新线程中运行。 如果并发请求过多，可能会导致资源耗尽。 建议在app.py中增加任务队列或并发控制机制，限制同时运行的爬虫任务数量。 项目文件详解 app.py Flask应用入口，提供了/crawl POST接口。 功能： 接收请求参数：user_id、cookie、start_date、end_date等。 启动一个新线程运行爬虫，避免阻塞API响应。 返回JSON格式的响应，告知请求已开始处理。 weibo.py 核心爬虫代码，定义了WeiboCrawler类。 功能： 构造请求，获取微博数据。 解析微博内容、用户信息、图片等。 支持长微博处理，获取完整内容。 可选择下载微博中的图片。 utils.py 工具函数，包含： datatype_process：数据类型转换，处理常见的数据格式。 convert_date_to_timestamp：将日期字符串转换为时间戳。 config.py 配置文件，包含全局配置参数。 在Docker部署中，cookie和user_id等参数通过API接口传入。 main_single_user.py 主程序，抓取单个用户的微博数据。 使用示例中，替换user_id和cookie为目标用户信息即可。 main_from_xlsx.py 主程序，批量抓取多个用户的微博数据。 从指定的Excel文件中读取用户ID列表。 requirements.txt Python依赖库列表，可使用pip install -r requirements.txt安装。 Dockerfile Docker镜像构建文件。 指定了基础镜像、工作目录、依赖安装和启动命令。 weibo_results/ 数据保存目录。 程序运行后会自动创建，存储抓取的微博数据和图片。 常见问题 1. 爬虫无法抓取数据，提示Cookie错误 可能是Cookie过期，请重新获取并更新config.py中的cookie变量，或在API请求中传入最新的Cookie。 2. 运行程序时出现SSL错误 可能是网络问题或请求频率过高导致。 尝试降低请求频率，或在weibo.py中调整verify=False来忽略SSL验证（仅用于调试，不推荐长期使用）。 3. 如何获取用户ID 打开微博用户主页，浏览器地址栏中的一串数字即为用户ID。 也可以通过在线工具或API转换用户名为用户ID。 4. Docker容器无法启动或运行异常 请确保Docker已正确安装并正在运行。 检查Dockerfile和docker run命令的参数是否正确。 查看容器日志以获取更多错误信息：docker logs -f weibo-crawler 联系方式 如有问题或建议，欢迎联系项目作者。\n作者：Junwen Yang 邮箱：lucas.junwen.yang@gmail.com 免责声明 本项目仅供学习和研究使用，禁止用于任何商业或非法用途。 使用本工具产生的任何风险和后果由用户自行承担，作者不承担任何责任。 ",
  "wordCount" : "3431",
  "inLanguage": "en",
  "datePublished": "2024-10-13T00:00:00Z",
  "dateModified": "2024-10-13T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://AquariniqueMu.github.io/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JW's Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://AquariniqueMu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://AquariniqueMu.github.io/" accesskey="h" title="JW&#39;s Log (Alt + H)">JW&#39;s Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://AquariniqueMu.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://AquariniqueMu.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://AquariniqueMu.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://AquariniqueMu.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://AquariniqueMu.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://AquariniqueMu.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      微博用户信息抓取手册
    </h1>
    <div class="post-description">
      主要介绍如何通过TODESK和SSH连接实验室的服务器系统
    </div>
    <div class="post-meta"><span title='2024-10-13 00:00:00 +0000 UTC'>October 13, 2024</span>&nbsp;·&nbsp;7 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%be%ae%e5%8d%9a%e7%94%a8%e6%88%b7%e7%88%ac%e8%99%ab%e9%a1%b9%e7%9b%ae" aria-label="微博用户爬虫项目">微博用户爬虫项目</a><ul>
                        
                <li>
                    <a href="#%e7%9b%ae%e5%bd%95" aria-label="目录">目录</a></li>
                <li>
                    <a href="#%e9%a1%b9%e7%9b%ae%e7%bb%93%e6%9e%84" aria-label="项目结构">项目结构</a></li>
                <li>
                    <a href="#%e9%a1%b9%e7%9b%ae%e4%b8%8b%e8%bd%bd" aria-label="项目下载">项目下载</a></li>
                <li>
                    <a href="#%e7%8e%af%e5%a2%83%e4%be%9d%e8%b5%96" aria-label="环境依赖">环境依赖</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6-configpy" aria-label="配置文件 config.py">配置文件 config.py</a></li>
                <li>
                    <a href="#%e8%bf%90%e8%a1%8c%e7%88%ac%e8%99%ab" aria-label="运行爬虫">运行爬虫</a><ul>
                        
                <li>
                    <a href="#1-%e4%bd%bf%e7%94%a8docker%e9%83%a8%e7%bd%b2%e5%92%8c%e8%bf%90%e8%a1%8c" aria-label="1. 使用Docker部署和运行">1. 使用Docker部署和运行</a><ul>
                        
                <li>
                    <a href="#11-%e6%9e%84%e5%bb%badocker%e9%95%9c%e5%83%8f" aria-label="1.1 构建Docker镜像">1.1 构建Docker镜像</a></li>
                <li>
                    <a href="#12-%e8%bf%90%e8%a1%8cdocker%e5%ae%b9%e5%99%a8" aria-label="1.2 运行Docker容器">1.2 运行Docker容器</a></li>
                <li>
                    <a href="#13-%e8%b0%83%e7%94%a8api%e6%8e%a5%e5%8f%a3" aria-label="1.3 调用API接口">1.3 调用API接口</a></li>
                <li>
                    <a href="#14-%e6%9f%a5%e7%9c%8b%e7%bb%93%e6%9e%9c" aria-label="1.4 查看结果">1.4 查看结果</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e7%9b%b4%e6%8e%a5%e8%bf%90%e8%a1%8cpython%e8%84%9a%e6%9c%ac" aria-label="2. 直接运行Python脚本">2. 直接运行Python脚本</a><ul>
                        
                <li>
                    <a href="#21-%e6%8a%93%e5%8f%96%e5%8d%95%e4%b8%aa%e7%94%a8%e6%88%b7%e5%be%ae%e5%8d%9a%e6%95%b0%e6%8d%ae" aria-label="2.1 抓取单个用户微博数据">2.1 抓取单个用户微博数据</a></li>
                <li>
                    <a href="#22-%e6%89%b9%e9%87%8f%e6%8a%93%e5%8f%96%e5%a4%9a%e4%b8%aa%e7%94%a8%e6%88%b7%e5%be%ae%e5%8d%9a%e6%95%b0%e6%8d%ae" aria-label="2.2 批量抓取多个用户微博数据">2.2 批量抓取多个用户微博数据</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8" aria-label="数据存储">数据存储</a></li>
                <li>
                    <a href="#%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9" aria-label="注意事项">注意事项</a><ul>
                        
                <li>
                    <a href="#%e5%90%88%e6%b3%95%e6%80%a7%e4%b8%8e%e5%90%88%e8%a7%84%e6%80%a7" aria-label="合法性与合规性">合法性与合规性</a></li>
                <li>
                    <a href="#%e5%85%b3%e4%ba%8ecookie" aria-label="关于Cookie">关于Cookie</a></li>
                <li>
                    <a href="#%e8%af%b7%e6%b1%82%e9%a2%91%e7%8e%87" aria-label="请求频率">请求频率</a></li>
                <li>
                    <a href="#%e5%a4%9a%e7%ba%bf%e7%a8%8b%e4%b8%8e%e5%b9%b6%e5%8f%91" aria-label="多线程与并发">多线程与并发</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%a1%b9%e7%9b%ae%e6%96%87%e4%bb%b6%e8%af%a6%e8%a7%a3" aria-label="项目文件详解">项目文件详解</a><ul>
                        
                <li>
                    <a href="#apppy" aria-label="app.py">app.py</a></li>
                <li>
                    <a href="#weibopy" aria-label="weibo.py">weibo.py</a></li>
                <li>
                    <a href="#utilspy" aria-label="utils.py">utils.py</a></li>
                <li>
                    <a href="#configpy" aria-label="config.py">config.py</a></li>
                <li>
                    <a href="#main_single_userpy" aria-label="main_single_user.py">main_single_user.py</a></li>
                <li>
                    <a href="#main_from_xlsxpy" aria-label="main_from_xlsx.py">main_from_xlsx.py</a></li>
                <li>
                    <a href="#requirementstxt" aria-label="requirements.txt">requirements.txt</a></li>
                <li>
                    <a href="#dockerfile" aria-label="Dockerfile">Dockerfile</a></li>
                <li>
                    <a href="#weibo_results" aria-label="weibo_results/">weibo_results/</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98" aria-label="常见问题">常见问题</a><ul>
                        
                <li>
                    <a href="#1-%e7%88%ac%e8%99%ab%e6%97%a0%e6%b3%95%e6%8a%93%e5%8f%96%e6%95%b0%e6%8d%ae%e6%8f%90%e7%a4%bacookie%e9%94%99%e8%af%af" aria-label="1. 爬虫无法抓取数据，提示Cookie错误">1. 爬虫无法抓取数据，提示Cookie错误</a></li>
                <li>
                    <a href="#2-%e8%bf%90%e8%a1%8c%e7%a8%8b%e5%ba%8f%e6%97%b6%e5%87%ba%e7%8e%b0ssl%e9%94%99%e8%af%af" aria-label="2. 运行程序时出现SSL错误">2. 运行程序时出现SSL错误</a></li>
                <li>
                    <a href="#3-%e5%a6%82%e4%bd%95%e8%8e%b7%e5%8f%96%e7%94%a8%e6%88%b7id" aria-label="3. 如何获取用户ID">3. 如何获取用户ID</a></li>
                <li>
                    <a href="#4-docker%e5%ae%b9%e5%99%a8%e6%97%a0%e6%b3%95%e5%90%af%e5%8a%a8%e6%88%96%e8%bf%90%e8%a1%8c%e5%bc%82%e5%b8%b8" aria-label="4. Docker容器无法启动或运行异常">4. Docker容器无法启动或运行异常</a></li></ul>
                </li>
                <li>
                    <a href="#%e8%81%94%e7%b3%bb%e6%96%b9%e5%bc%8f" aria-label="联系方式">联系方式</a></li>
                <li>
                    <a href="#%e5%85%8d%e8%b4%a3%e5%a3%b0%e6%98%8e" aria-label="免责声明">免责声明</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="微博用户爬虫项目">微博用户爬虫项目<a hidden class="anchor" aria-hidden="true" href="#微博用户爬虫项目">#</a></h1>
<p>本项目是一个用于批量抓取微博用户微博数据的爬虫工具，支持从Excel文件中读取用户ID，抓取指定时间范围内的微博内容、图片等信息，并将结果保存到本地。现在，我们已经将项目封装为Docker容器，并开放了一个API接口，方便他人部署和使用。</p>
<p>地址：https://github.com/AquariniqueMu/WeiboUserCrawler</p>
<h2 id="目录">目录<a hidden class="anchor" aria-hidden="true" href="#目录">#</a></h2>
<ul>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#项目结构">项目结构</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#环境依赖">环境依赖</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#配置文件-configpy">配置文件 <code>config.py</code></a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#运行爬虫">运行爬虫</a>
<ul>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#1-使用docker部署和运行">1. 使用Docker部署和运行</a>
<ul>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#11-构建docker镜像">1.1 构建Docker镜像</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#12-运行docker容器">1.2 运行Docker容器</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#13-调用api接口">1.3 调用API接口</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#14-查看结果">1.4 查看结果</a></li>
</ul>
</li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#2-直接运行python脚本">2. 直接运行Python脚本</a>
<ul>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#21-抓取单个用户微博数据">2.1 抓取单个用户微博数据</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#22-批量抓取多个用户微博数据">2.2 批量抓取多个用户微博数据</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#数据存储">数据存储</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#注意事项">注意事项</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#项目文件详解">项目文件详解</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#常见问题">常见问题</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#联系方式">联系方式</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#免责声明">免责声明</a></li>
<li><a href="/posts/%E5%BE%AE%E5%8D%9A%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%8A%93%E5%8F%96%E6%89%8B%E5%86%8C/#requirementstxt">requirements.txt</a></li>
</ul>
<hr>
<h2 id="项目结构">项目结构<a hidden class="anchor" aria-hidden="true" href="#项目结构">#</a></h2>
<pre tabindex="0"><code>WeiboUserCrawler/
├── app.py
├── config.py
├── Dockerfile
├── main_from_xlsx.py
├── main_single_user.py
├── requirements.txt
├── utils.py
├── weibo.py
└── weibo_results/ (empty, will be populated during runtime)
</code></pre><ul>
<li><code>app.py</code>：Flask应用入口，提供API接口。</li>
<li><code>config.py</code>：配置文件，包含默认参数。</li>
<li><code>Dockerfile</code>：Docker镜像构建文件。</li>
<li><code>main_from_xlsx.py</code>：从Excel文件中读取用户ID，批量抓取微博数据的脚本。</li>
<li><code>main_single_user.py</code>：抓取单个用户微博数据的脚本。</li>
<li><code>requirements.txt</code>：Python依赖库列表。</li>
<li><code>utils.py</code>：工具函数，包含数据类型处理和日期转换等功能。</li>
<li><code>weibo.py</code>：微博爬虫核心代码，定义了<code>WeiboCrawler</code>类。</li>
<li><code>weibo_results/</code>：默认的数据保存目录，抓取的结果会存储在此文件夹中。</li>
</ul>
<hr>
<h2 id="项目下载">项目下载<a hidden class="anchor" aria-hidden="true" href="#项目下载">#</a></h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/AquariniqueMu/WeiboUserCrawler.git
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> WeiboUserCrawler
</span></span></code></pre></div><h2 id="环境依赖">环境依赖<a hidden class="anchor" aria-hidden="true" href="#环境依赖">#</a></h2>
<ul>
<li>
<p>Python 版本：<strong>3.12.5</strong></p>
</li>
<li>
<p>依赖库：</p>
<pre tabindex="0"><code>requests&gt;=2.31.0
pandas&gt;=2.0.3
tqdm&gt;=4.65.0
openpyxl&gt;=3.1.2
Flask&gt;=2.0.3
</code></pre><p>可使用以下命令安装依赖：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install -r requirements.txt
</span></span></code></pre></div></li>
</ul>
<hr>
<h2 id="配置文件-configpy">配置文件 <code>config.py</code><a hidden class="anchor" aria-hidden="true" href="#配置文件-configpy">#</a></h2>
<p>在使用爬虫前，需要配置<code>config.py</code>文件，包括Cookie、抓取时间范围等。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">convert_date_to_timestamp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 默认的Cookie，可在运行时通过API传入</span>
</span></span><span class="line"><span class="cl"><span class="n">cookie</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">result_dir</span> <span class="o">=</span> <span class="s1">&#39;./weibo_results&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">start_date</span> <span class="o">=</span> <span class="n">convert_date_to_timestamp</span><span class="p">(</span><span class="s1">&#39;2023-01-01&#39;</span><span class="p">)</span>  <span class="c1"># 开始时间，格式：YYYY-MM-DD</span>
</span></span><span class="line"><span class="cl"><span class="n">end_date</span> <span class="o">=</span> <span class="n">convert_date_to_timestamp</span><span class="p">(</span><span class="s1">&#39;2023-12-31&#39;</span><span class="p">)</span>    <span class="c1"># 结束时间，格式：YYYY-MM-DD</span>
</span></span><span class="line"><span class="cl"><span class="n">download_images</span> <span class="o">=</span> <span class="kc">True</span>     <span class="c1"># 是否下载图片</span>
</span></span><span class="line"><span class="cl"><span class="n">fetch_reposts</span> <span class="o">=</span> <span class="kc">True</span>       <span class="c1"># 是否抓取转发微博</span>
</span></span></code></pre></div><hr>
<h2 id="运行爬虫">运行爬虫<a hidden class="anchor" aria-hidden="true" href="#运行爬虫">#</a></h2>
<h3 id="1-使用docker部署和运行">1. 使用Docker部署和运行<a hidden class="anchor" aria-hidden="true" href="#1-使用docker部署和运行">#</a></h3>
<p>通过Docker容器化，您可以轻松地在任何支持Docker的环境中部署和运行本项目。</p>
<h4 id="11-构建docker镜像">1.1 构建Docker镜像<a hidden class="anchor" aria-hidden="true" href="#11-构建docker镜像">#</a></h4>
<p>在项目根目录下，执行以下命令构建Docker镜像：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker build -t weibo-crawler .
</span></span></code></pre></div><h4 id="12-运行docker容器">1.2 运行Docker容器<a hidden class="anchor" aria-hidden="true" href="#12-运行docker容器">#</a></h4>
<p>使用以下命令运行Docker容器，并将结果目录映射到宿主机：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker run -d -p 5000:5000 -v /your/host/path/weibo_results:/app/weibo_results --name weibo-crawler weibo-crawler
</span></span></code></pre></div><ul>
<li><code>-d</code>：以守护进程方式运行容器。</li>
<li><code>-p 5000:5000</code>：将容器内的5000端口映射到宿主机的5000端口。</li>
<li><code>-v /your/host/path/weibo_results:/app/weibo_results</code>：将宿主机的目录映射到容器内的<code>/app/weibo_results</code>，用于数据持久化。</li>
<li><code>--name weibo-crawler</code>：为容器指定一个名称，方便管理。</li>
</ul>
<p>**注意：**请将<code>/your/host/path/weibo_results</code>替换为您宿主机上用于存放结果的实际路径。</p>
<h4 id="13-调用api接口">1.3 调用API接口<a hidden class="anchor" aria-hidden="true" href="#13-调用api接口">#</a></h4>
<p>使用<code>curl</code>命令或其他HTTP客户端（如Postman）发送POST请求：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl -X POST http://localhost:5000/crawl <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     -H <span class="s2">&#34;Content-Type: application/json&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>     -d <span class="s1">&#39;{
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;user_id&#34;: &#34;1234567890&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;cookie&#34;: &#34;YOUR_COOKIE_HERE&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;start_date&#34;: &#34;2023-01-01&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;end_date&#34;: &#34;2023-12-31&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;download_images&#34;: true,
</span></span></span><span class="line"><span class="cl"><span class="s1">           &#34;fetch_reposts&#34;: true
</span></span></span><span class="line"><span class="cl"><span class="s1">         }&#39;</span>
</span></span></code></pre></div><p><strong>参数说明：</strong></p>
<ul>
<li><code>user_id</code>：必填，微博用户ID。</li>
<li><code>cookie</code>：必填，您的微博Cookie。</li>
<li><code>start_date</code>：选填，开始日期，格式<code>YYYY-MM-DD</code>。</li>
<li><code>end_date</code>：选填，结束日期，格式<code>YYYY-MM-DD</code>。</li>
<li><code>download_images</code>：选填，是否下载图片，默认为<code>false</code>。</li>
<li><code>fetch_reposts</code>：选填，是否抓取转发微博，默认为<code>false</code>。</li>
</ul>
<p><strong>示例响应：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;message&#34;</span><span class="p">:</span> <span class="s2">&#34;Started crawling for user 1234567890&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h4 id="14-查看结果">1.4 查看结果<a hidden class="anchor" aria-hidden="true" href="#14-查看结果">#</a></h4>
<p>抓取的结果会保存在宿主机的<code>/your/host/path/weibo_results</code>目录下，以用户ID命名的子目录中。</p>
<p><strong>查看容器日志：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker logs -f weibo-crawler
</span></span></code></pre></div><p><strong>停止容器：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker stop weibo-crawler
</span></span></code></pre></div><p><strong>启动容器：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker start weibo-crawler
</span></span></code></pre></div><p><strong>删除容器：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker rm weibo-crawler
</span></span></code></pre></div><hr>
<h3 id="2-直接运行python脚本">2. 直接运行Python脚本<a hidden class="anchor" aria-hidden="true" href="#2-直接运行python脚本">#</a></h3>
<p>如果您希望在本地直接运行Python脚本，可以按照以下步骤操作。</p>
<h4 id="21-抓取单个用户微博数据">2.1 抓取单个用户微博数据<a hidden class="anchor" aria-hidden="true" href="#21-抓取单个用户微博数据">#</a></h4>
<p>修改<code>main_single_user.py</code>中的<code>user_id</code>为目标用户的ID，然后运行该脚本。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">weibo</span> <span class="kn">import</span> <span class="n">WeiboCrawler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">config</span> <span class="kn">import</span> <span class="n">cookie</span><span class="p">,</span> <span class="n">result_dir</span><span class="p">,</span> <span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">,</span> <span class="n">download_images</span><span class="p">,</span> <span class="n">fetch_reposts</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_weibo_crawler</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">user_id</span> <span class="o">=</span> <span class="s1">&#39;1234567890&#39;</span>  <span class="c1"># 替换为目标用户的ID</span>
</span></span><span class="line"><span class="cl">    <span class="n">cookie</span> <span class="o">=</span> <span class="s1">&#39;YOUR_COOKIE_HERE&#39;</span>  <span class="c1"># 替换为你的Cookie</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;user_id&#39;</span><span class="p">:</span> <span class="n">user_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;cookie&#39;</span><span class="p">:</span> <span class="n">cookie</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;result_dir&#39;</span><span class="p">:</span> <span class="n">result_dir</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;start_date&#39;</span><span class="p">:</span> <span class="n">start_date</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;end_date&#39;</span><span class="p">:</span> <span class="n">end_date</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;download_images&#39;</span><span class="p">:</span> <span class="n">download_images</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;fetch_reposts&#39;</span><span class="p">:</span> <span class="n">fetch_reposts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="s2">&#34; Start Weibo Crawler &#34;</span> <span class="o">+</span> <span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">weibo_crawler</span> <span class="o">=</span> <span class="n">WeiboCrawler</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">weibo_crawler</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="s2">&#34; End Weibo Crawler &#34;</span> <span class="o">+</span> <span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">run_weibo_crawler</span><span class="p">()</span>
</span></span></code></pre></div><p><strong>运行命令：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python main_single_user.py
</span></span></code></pre></div><h4 id="22-批量抓取多个用户微博数据">2.2 批量抓取多个用户微博数据<a hidden class="anchor" aria-hidden="true" href="#22-批量抓取多个用户微博数据">#</a></h4>
<p>将用户ID列表保存在Excel文件中，然后运行<code>main_from_xlsx.py</code>。</p>
<p><strong>准备Excel文件</strong></p>
<ul>
<li>创建一个Excel文件（默认文件名：<code>user_ids.xlsx</code>）。</li>
<li>在第一列（默认列名：<code>用户ID</code>）中填写用户ID列表。</li>
</ul>
<p><strong>运行脚本</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python main_from_xlsx.py --excel_file your_excel_file.xlsx --id_column 用户ID
</span></span></code></pre></div><ul>
<li><code>--excel_file</code>：Excel文件路径，默认值为<code>user_ids.xlsx</code>。</li>
<li><code>--id_column</code>：用户ID列名，默认值为<code>用户ID</code>。</li>
</ul>
<p><strong>示例：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python main_from_xlsx.py
</span></span></code></pre></div><p>如果使用默认的文件名和列名，可以直接运行上述命令。</p>
<hr>
<h2 id="数据存储">数据存储<a hidden class="anchor" aria-hidden="true" href="#数据存储">#</a></h2>
<ul>
<li>所有抓取的结果会存储在<code>weibo_results/</code>目录下。</li>
<li>每个用户的数据会存放在以用户ID命名的子目录中。</li>
<li>数据包括：
<ul>
<li>微博内容CSV文件：<code>weibo_用户ID.csv</code></li>
<li>图片文件：存储在对应微博ID的<code>images/</code>目录下。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="注意事项">注意事项<a hidden class="anchor" aria-hidden="true" href="#注意事项">#</a></h2>
<h3 id="合法性与合规性">合法性与合规性<a hidden class="anchor" aria-hidden="true" href="#合法性与合规性">#</a></h3>
<ul>
<li><strong>请遵守微博的服务条款和相关法律法规。</strong></li>
<li>本工具仅供学习和研究使用，禁止用于任何商业或非法用途。</li>
<li>请勿过度抓取，以免给微博服务器带来压力。</li>
</ul>
<h3 id="关于cookie">关于Cookie<a hidden class="anchor" aria-hidden="true" href="#关于cookie">#</a></h3>
<ul>
<li><strong>安全性</strong>：Cookie包含敏感信息，请妥善保管，避免泄露。</li>
<li><strong>时效性</strong>：Cookie可能会过期，若爬虫无法正常工作，请尝试更新Cookie。</li>
</ul>
<h3 id="请求频率">请求频率<a hidden class="anchor" aria-hidden="true" href="#请求频率">#</a></h3>
<ul>
<li>为避免触发微博的反爬机制，程序中已设置了随机等待时间。</li>
<li>如需调整，请在<code>weibo.py</code>的<code>start</code>方法中修改<code>time.sleep</code>的参数。</li>
</ul>
<h3 id="多线程与并发">多线程与并发<a hidden class="anchor" aria-hidden="true" href="#多线程与并发">#</a></h3>
<ul>
<li>Docker部署的API接口中，每个爬虫任务在一个新线程中运行。</li>
<li>如果并发请求过多，可能会导致资源耗尽。</li>
<li>建议在<code>app.py</code>中增加任务队列或并发控制机制，限制同时运行的爬虫任务数量。</li>
</ul>
<hr>
<h2 id="项目文件详解">项目文件详解<a hidden class="anchor" aria-hidden="true" href="#项目文件详解">#</a></h2>
<h3 id="apppy"><code>app.py</code><a hidden class="anchor" aria-hidden="true" href="#apppy">#</a></h3>
<ul>
<li>Flask应用入口，提供了<code>/crawl</code> POST接口。</li>
<li>功能：
<ul>
<li>接收请求参数：<code>user_id</code>、<code>cookie</code>、<code>start_date</code>、<code>end_date</code>等。</li>
<li>启动一个新线程运行爬虫，避免阻塞API响应。</li>
<li>返回JSON格式的响应，告知请求已开始处理。</li>
</ul>
</li>
</ul>
<h3 id="weibopy"><code>weibo.py</code><a hidden class="anchor" aria-hidden="true" href="#weibopy">#</a></h3>
<ul>
<li>核心爬虫代码，定义了<code>WeiboCrawler</code>类。</li>
<li>功能：
<ul>
<li>构造请求，获取微博数据。</li>
<li>解析微博内容、用户信息、图片等。</li>
<li>支持长微博处理，获取完整内容。</li>
<li>可选择下载微博中的图片。</li>
</ul>
</li>
</ul>
<h3 id="utilspy"><code>utils.py</code><a hidden class="anchor" aria-hidden="true" href="#utilspy">#</a></h3>
<ul>
<li>工具函数，包含：
<ul>
<li><code>datatype_process</code>：数据类型转换，处理常见的数据格式。</li>
<li><code>convert_date_to_timestamp</code>：将日期字符串转换为时间戳。</li>
</ul>
</li>
</ul>
<h3 id="configpy"><code>config.py</code><a hidden class="anchor" aria-hidden="true" href="#configpy">#</a></h3>
<ul>
<li>配置文件，包含全局配置参数。</li>
<li>在Docker部署中，<code>cookie</code>和<code>user_id</code>等参数通过API接口传入。</li>
</ul>
<h3 id="main_single_userpy"><code>main_single_user.py</code><a hidden class="anchor" aria-hidden="true" href="#main_single_userpy">#</a></h3>
<ul>
<li>主程序，抓取单个用户的微博数据。</li>
<li>使用示例中，替换<code>user_id</code>和<code>cookie</code>为目标用户信息即可。</li>
</ul>
<h3 id="main_from_xlsxpy"><code>main_from_xlsx.py</code><a hidden class="anchor" aria-hidden="true" href="#main_from_xlsxpy">#</a></h3>
<ul>
<li>主程序，批量抓取多个用户的微博数据。</li>
<li>从指定的Excel文件中读取用户ID列表。</li>
</ul>
<h3 id="requirementstxt"><code>requirements.txt</code><a hidden class="anchor" aria-hidden="true" href="#requirementstxt">#</a></h3>
<ul>
<li>Python依赖库列表，可使用<code>pip install -r requirements.txt</code>安装。</li>
</ul>
<h3 id="dockerfile"><code>Dockerfile</code><a hidden class="anchor" aria-hidden="true" href="#dockerfile">#</a></h3>
<ul>
<li>Docker镜像构建文件。</li>
<li>指定了基础镜像、工作目录、依赖安装和启动命令。</li>
</ul>
<h3 id="weibo_results"><code>weibo_results/</code><a hidden class="anchor" aria-hidden="true" href="#weibo_results">#</a></h3>
<ul>
<li>数据保存目录。</li>
<li>程序运行后会自动创建，存储抓取的微博数据和图片。</li>
</ul>
<hr>
<h2 id="常见问题">常见问题<a hidden class="anchor" aria-hidden="true" href="#常见问题">#</a></h2>
<h3 id="1-爬虫无法抓取数据提示cookie错误">1. 爬虫无法抓取数据，提示Cookie错误<a hidden class="anchor" aria-hidden="true" href="#1-爬虫无法抓取数据提示cookie错误">#</a></h3>
<ul>
<li>可能是Cookie过期，请重新获取并更新<code>config.py</code>中的<code>cookie</code>变量，或在API请求中传入最新的Cookie。</li>
</ul>
<h3 id="2-运行程序时出现ssl错误">2. 运行程序时出现SSL错误<a hidden class="anchor" aria-hidden="true" href="#2-运行程序时出现ssl错误">#</a></h3>
<ul>
<li>可能是网络问题或请求频率过高导致。</li>
<li>尝试降低请求频率，或在<code>weibo.py</code>中调整<code>verify=False</code>来忽略SSL验证（仅用于调试，不推荐长期使用）。</li>
</ul>
<h3 id="3-如何获取用户id">3. 如何获取用户ID<a hidden class="anchor" aria-hidden="true" href="#3-如何获取用户id">#</a></h3>
<ul>
<li>打开微博用户主页，浏览器地址栏中的一串数字即为用户ID。</li>
<li>也可以通过在线工具或API转换用户名为用户ID。</li>
</ul>
<h3 id="4-docker容器无法启动或运行异常">4. Docker容器无法启动或运行异常<a hidden class="anchor" aria-hidden="true" href="#4-docker容器无法启动或运行异常">#</a></h3>
<ul>
<li>请确保Docker已正确安装并正在运行。</li>
<li>检查<code>Dockerfile</code>和<code>docker run</code>命令的参数是否正确。</li>
<li>查看容器日志以获取更多错误信息：<code>docker logs -f weibo-crawler</code></li>
</ul>
<hr>
<h2 id="联系方式">联系方式<a hidden class="anchor" aria-hidden="true" href="#联系方式">#</a></h2>
<p>如有问题或建议，欢迎联系项目作者。</p>
<ul>
<li>作者：Junwen Yang</li>
<li>邮箱：lucas.junwen.yang@gmail.com</li>
</ul>
<hr>
<h2 id="免责声明">免责声明<a hidden class="anchor" aria-hidden="true" href="#免责声明">#</a></h2>
<ul>
<li>本项目仅供学习和研究使用，禁止用于任何商业或非法用途。</li>
<li>使用本工具产生的任何风险和后果由用户自行承担，作者不承担任何责任。</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://AquariniqueMu.github.io/tags/%E7%88%AC%E8%99%AB/">爬虫</a></li>
      <li><a href="https://AquariniqueMu.github.io/tags/%E5%BE%AE%E5%8D%9A/">微博</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://AquariniqueMu.github.io/posts/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E4%B8%BB%E6%9C%BA%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/">
    <span class="title">Next »</span>
    <br>
    <span>实验室主机连接手册</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://AquariniqueMu.github.io/">©2024 Junwen&rsquo;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
